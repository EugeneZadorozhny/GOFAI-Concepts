{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [str(value) for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(\"data/agaricus-lepiota.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\n",
    "    \"cap-shape\",\n",
    "    \"cap-surface\",\n",
    "    \"cap-color\",\n",
    "    \"bruises?\",\n",
    "    \"odor\",\n",
    "    \"gill-attachment\",\n",
    "    \"gill-spacing\",\n",
    "    \"gill-size\",\n",
    "    \"gill-color\",\n",
    "    \"stalk-shape\",\n",
    "    \"stalk-root\",\n",
    "    \"stalk-surface-above-ring\",\n",
    "    \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\",\n",
    "    \"stalk-color-below-ring\",\n",
    "    \"veil-type\",\n",
    "    \"veil-color\",\n",
    "    \"ring-number\",\n",
    "    \"ring-type\",\n",
    "    \"spore-print-color\",\n",
    "    \"population\",\n",
    "    \"habitat\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extract_column\"></a>\n",
    "### extract_column\n",
    "\n",
    "`extract_column` extracts a column into a list from a list of lists. **Used by**:[c_class](#c_class), [evaluate](#evaluate)\n",
    "\n",
    "* **data** List[List]: the list of lists\n",
    "* **column** int: determines which column to extract\n",
    "\n",
    "**return**: List: the extracted column as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_column(data: List[List], column) -> List:\n",
    "    extract = []\n",
    "    for i, value in enumerate(data):\n",
    "        extract.append(data[i][column])\n",
    "    return extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_folds\"></a>\n",
    "### create_folds\n",
    "\n",
    "`create_folds` creates folds of data to use for cross_validations. **Used by**:[cross_validate](#cross_validate)\n",
    "\n",
    "* **xs** List: the list to create folds with\n",
    "* **n** int: number of folds\n",
    "\n",
    "**return**: List[List[List]]: list of folds stored as list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    return list(xs[i * k + min(i, m) : (i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_train_test\"></a>\n",
    "### create_train_test\n",
    "\n",
    "`create_train_test` creates a training set and a test set from folded data. **Used by**: [cross_validate](#cross_validate)\n",
    "\n",
    "* **folds** List[List[List]]: the list of folded data\n",
    "* **index** int: which fold to use as test data\n",
    "\n",
    "**return**: [List[List[List], List]: One test data list, and the rest of the data as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(\n",
    "    folds: List[List[List]], index: int\n",
    ") -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "### train\n",
    "\n",
    "`train` Runs the training algorithm and returns the model. **Used by**: [cross_validate](#cross_validate) **Uses**: [clf_nb](#clf_nb)\n",
    "\n",
    "* **training_data** List[List[List]]: a list of trainging data\n",
    "* **attributes** List: list of attributes that the data represents\n",
    "* **smoothing** Bool: determines whether to use +1 smoothing or not\n",
    "\n",
    "**return**: Dict: the model of probabilities that can be used to classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, attributes, smoothing=True) -> Dict:\n",
    "    model = clf_nb(training_data, attributes, smoothing)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classify\"></a>\n",
    "### classify\n",
    "\n",
    "`classify` Runs the classification algorithm to attach labels to data based on the Naive Bayes model. **Used by**:[cross_validate](#cross_validate) **Uses**: [classify_nb](#classify_nb)\n",
    "\n",
    "* **nbc** Dict: the Naive Bayes model\n",
    "* **observations** List[List]: The list of data to be classified\n",
    "* **attributes** List: list of attributes that the data represents\n",
    "* **Labeled** Bool: determines whether the data is labeled or only contains features\n",
    "\n",
    "**return**: List[Tuple]: The list of data labeled using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(nbc, observations, attributes, labeled=True) -> List:\n",
    "    result = []\n",
    "    if isinstance(observations[0], list):\n",
    "        for obs in observations:\n",
    "            instance_result = classify_nb(nbc, obs, attributes, labeled)\n",
    "            result.append(instance_result)\n",
    "    else:\n",
    "        instance_result = classify_nb(nbc, observations, attributes, labeled)\n",
    "        result.append(instance_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classify_nb\"></a>\n",
    "### classify_nb\n",
    "\n",
    "`classify_nb` Runs the classification algorithm to attach labels to an instance of data based on the Naive Bayes model. **Used by**:[classify](#classify) **Uses**:[probability_of](#probability_of), [normalize](#normalize)\n",
    "\n",
    "* **probs** Dict: the Naive Bayes model\n",
    "* **instance** List: The list of data to be classified\n",
    "* **attributes** List: list of attributes that the data represents\n",
    "* **labeled** Bool: determines whether the data is labeled or only contains features\n",
    "\n",
    "**return**: Tuple: The best value found and the probabilities of the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_nb(probs, instance, attributes, labeled):\n",
    "    result = {}\n",
    "    best = 0\n",
    "    class_labels = list(probs[\"class\"].keys())\n",
    "    for label in class_labels:\n",
    "        result[label] = probability_of(instance, label, probs, attributes, labeled)\n",
    "    result = normalize(result)\n",
    "    for k in result:\n",
    "        if result[k] > best:\n",
    "            best = result[k]\n",
    "            best_val = k\n",
    "    return (best_val, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"probability_of\"></a>\n",
    "### probability_of\n",
    "\n",
    "`classify_nb` Runs the classification algorithm to attach labels to an instance of data based on the Naive Bayes model. **Used by**:[classify_nb](#classify_nb) \n",
    "\n",
    "* **probs** Dict: the Naive Bayes model\n",
    "* **label** str: the class label to be evaluated for\n",
    "* **instance** List: The list of data to be classified\n",
    "* **attributes** List: list of attributes that the data represents\n",
    "* **labeled** Bool: determines whether the data is labeled or only contains features\n",
    "\n",
    "**return**: float: the probability of the instance matching the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of(instance, label, probs, attributes, labeled):\n",
    "    probability = 1\n",
    "    for i, a in enumerate(instance):\n",
    "        if labeled is True:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            probability = probability * probs[attributes[i - 1]][a][label]\n",
    "        else:\n",
    "            probability = probability * probs[attributes[i]][a][label]\n",
    "    probability = probability * probs[\"class\"][label]\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"normalize\"></a>\n",
    "### normalize\n",
    "\n",
    "`normalize` normalizes the probability for each class label. **Used by**:[classify_nb](#classify_nb) \n",
    "\n",
    "* **result** Dict: the probabilities for each class label for the instance\n",
    "\n",
    "**return**: Dict: the normalized probabilities for each class label for the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(result):\n",
    "    norm = 0\n",
    "    for k in result:\n",
    "        norm += result[k]\n",
    "    for k in result:\n",
    "        result[k] = result[k] / norm\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "### evaluate\n",
    "\n",
    "`evaluate` evaluates the classified data versus the actual result. **Used by**:[cross_validate](#cross_validate) **Uses**: [extract_column](#extract_column)\n",
    "\n",
    "* **data_set** Dict: the data_set that was classified\n",
    "* **classification_result** List[Tuple]: list of classifications for the data_set\n",
    "\n",
    "**return**: float: the percentage of data that was correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_set, classification_result) -> float:\n",
    "    correct = 0\n",
    "    actual = extract_column(data_set, 0)\n",
    "    for i, value in enumerate(actual):\n",
    "        if classification_result[i][0] != actual[i]:\n",
    "            correct += 1\n",
    "    result = 100 * correct / len(actual)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clf_nb\"></a>\n",
    "### clf_nb\n",
    "\n",
    "`clf_nb` Runs the Naive Bayes algorithm to collect the probabilties of the features as they relate to the class labels **Used by**:[train](#train) **Uses**:[p_class](#p_class), [p_feature](#p_feature)\n",
    "\n",
    "* **data** List[List]: the dataset as a list of lists\n",
    "* **attributes** List[Str]: list of attributes in the dataset \n",
    "* **smoothing** Bool: determines if smoothing is turned on or off\n",
    "\n",
    "**return**: Dict: the class labels and fea and their probability of occurance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_nb(data, attributes, smoothing=True) -> Dict:\n",
    "    p_c = p_class(data, 0)\n",
    "    p_fc = p_feature(data, attributes, 0, smoothing)\n",
    "    nb_dict = {\"class\": p_c, **p_fc}\n",
    "    return nb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p_feature\"></a>\n",
    "### p_feature\n",
    "\n",
    "`p_feature` finds the probability of occurence of each feature for each class label in the dataset. **Used by**:[clf_nb](#clf_nb)  **Uses**:[c_class](#c_class)\n",
    "\n",
    "* **data** List[List]: the dataset as a list of lists\n",
    "* **attributes** List[Str]: list of attributes in the dataset \n",
    "* **class_loc** int: determines which column to extract as the class labels\n",
    "* **smoothing** Bool: determines if smoothing is turned on or off\n",
    "\n",
    "**return**: Dict: the class labels and their probability of occurance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_feature(data, attributes, class_loc, smoothing):\n",
    "    p_dict = {}\n",
    "    c_counts = c_class(data, class_loc)\n",
    "    for i, a in enumerate(attributes):\n",
    "        p_dict[a] = {}\n",
    "        a_counts = c_class(data, i + 1)\n",
    "        for k2 in a_counts:\n",
    "            p_dict[a][k2] = {}\n",
    "            for k in c_counts:\n",
    "                p_dict[a][k2][k] = 0\n",
    "                for d in data:\n",
    "                    if d[i + 1] == k2 and d[0] == k:\n",
    "                        p_dict[a][k2][k] += 1\n",
    "                if smoothing is True:\n",
    "                    p_dict[a][k2][k] = (p_dict[a][k2][k] + 1) / (c_counts[k] + 1)\n",
    "                else:\n",
    "                    p_dict[a][k2][k] = p_dict[a][k2][k] / c_counts[k]\n",
    "    return p_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p_class\"></a>\n",
    "### p_class\n",
    "\n",
    "`p_class` finds the probability of occurence for each class label in the dataset. **Used by**:[clf_nb](#clf_nb)  **Uses**:[c_class](#c_class)\n",
    "\n",
    "* **data** List[List]: the dataset as a list of lists\n",
    "* **class_loc** int: determines which column to extract as the class labels\n",
    "\n",
    "**return**: Dict: the class labels and their probability of occurance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_class(data, class_loc) -> Dict:\n",
    "    class_dict = c_class(data, class_loc)\n",
    "    for k in class_dict:\n",
    "        class_dict[k] = class_dict[k] / len(data)\n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"c_class\"></a>\n",
    "### c_class\n",
    "\n",
    "`c_class` counts the occurence for each class label in the dataset. **Used by**: [p_class](#p_class), [p_feature](#p_feature) **Uses**:[extract_column](#extract_column)\n",
    "\n",
    "* **data** List[List]: the dataset as a list of lists\n",
    "* **class_loc** int: determines which column to extract as the class labels\n",
    "\n",
    "**return**: Dict: the class labels and their probability of occurance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_class(data, class_loc) -> int:\n",
    "    class_dict = {}\n",
    "    class_col = extract_column(data, class_loc)\n",
    "    unique = np.unique(class_col)\n",
    "    for u in unique:\n",
    "        class_dict[u] = 0\n",
    "        for c in class_col:\n",
    "            if c == u:\n",
    "                class_dict[u] += 1\n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cross_validate\"></a>\n",
    "### cross_validate\n",
    "\n",
    "`cross_validate` Runs the 10 fold cross validation of the Naive Bayes model. **Uses**:[create_train_test](#create_train_test), [train](#train), [classify](#classify), [evaluate](#evaluate)\n",
    "\n",
    "* **data** List[List]: the dataset as a list of lists\n",
    "* **attributes** List[Str]: list of attributes in the dataset \n",
    "* **classify_function** Callable: determines which classify function to run\n",
    "* **eval_function** Callable: determines which evaluation function to run\n",
    "* **smoothing** Bool: determines if smoothing is turned on or off\n",
    "\n",
    "**return**: Dict: the class labels and their probability of occurance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(\n",
    "    data, attributes, classify_function, eval_function, smoothing=True\n",
    ") -> Any:\n",
    "    folds = create_folds(data, 10)\n",
    "    eval_clf = []\n",
    "    total = 0\n",
    "    for i, fold in enumerate(folds):\n",
    "        train_data, test = create_train_test(folds, i)\n",
    "        model = train(train_data, attributes, smoothing)\n",
    "        clf = classify_function(model, test, attributes, True)\n",
    "        eval_clf.append(eval_function(test, clf))\n",
    "        print(f\"Fold {i+1} error rate {eval_clf[i]}%\")\n",
    "    total = sum(eval_clf) / len(eval_clf)\n",
    "    res = sum((i - total) ** 2 for i in eval_clf) / len(eval_clf)\n",
    "    print(f\"Average Value: {total}, Variance: {res}\")\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 error rate 0.12300123001230012%\n",
      "Fold 2 error rate 0.24600246002460024%\n",
      "Fold 3 error rate 0.12300123001230012%\n",
      "Fold 4 error rate 0.24600246002460024%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 error rate 0.49261083743842365%\n",
      "Fold 6 error rate 0.49261083743842365%\n",
      "Fold 7 error rate 0.6157635467980296%\n",
      "Fold 8 error rate 0.49261083743842365%\n",
      "Fold 9 error rate 0.3694581280788177%\n",
      "Fold 10 error rate 0.24630541871921183%\n",
      "Average Value: 0.3447366985985131, Variance: 0.026718583698396175\n"
     ]
    }
   ],
   "source": [
    "model = cross_validate(data, attributes, classify, evaluate, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 error rate 4.797047970479705%\n",
      "Fold 2 error rate 4.059040590405904%\n",
      "Fold 3 error rate 4.182041820418204%\n",
      "Fold 4 error rate 4.305043050430505%\n",
      "Fold 5 error rate 4.187192118226601%\n",
      "Fold 6 error rate 3.8177339901477834%\n",
      "Fold 7 error rate 4.926108374384237%\n",
      "Fold 8 error rate 5.0492610837438425%\n",
      "Fold 9 error rate 5.41871921182266%\n",
      "Fold 10 error rate 4.679802955665025%\n",
      "Average Value: 4.542199116572446, Variance: 0.2326946516402139\n"
     ]
    }
   ],
   "source": [
    "model = cross_validate(data, attributes, classify, evaluate, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with no smoothing seemed to have a lower error rate. This is likely due to the data have many points where a feature has all of one class label. When adding the smoothing, that feature no longer has a 0 probability of not being the correct class label. Smoothing will likely make the model more robust in terms of overfitting if more data is collected that is more diverse in it's feature distribution among the classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
