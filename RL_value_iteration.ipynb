{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Value Iteration\n",
    "\n",
    "These are the same maps from Module 1 but the \"physics\" of the world have changed. In Module 1, the world was deterministic. When the agent moved \"south\", it went \"south\". When it moved \"east\", it went \"east\". Now, the agent only succeeds in going where it wants to go *sometimes*. There is a probability distribution over the possible states so that when the agent moves \"south\", there is a small probability that it will go \"east\", \"north\", or \"west\" instead and have to move from there.\n",
    "\n",
    "There are a variety of ways to handle this problem. For example, if using A\\* search, if the agent finds itself off the solution, you can simply calculate a new solution from where the agent ended up. Although this sounds like a really bad idea, it has actually been shown to work really well in video games that use formal planning algorithms (which we will cover later). When these algorithms were first designed, this was unthinkable. Thank you, Moore's Law!\n",
    "\n",
    "Another approach is to use Reinforcement Learning which covers problems where there is some kind of general uncertainty in the actions. We're going to model that uncertainty a bit unrealistically here but it'll show you how the algorithm works.\n",
    "\n",
    "As far as RL is concerned, there are a variety of options there: model-based and model-free, Value Iteration, Q-Learning and SARSA. You are going to use Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The World Representation\n",
    "\n",
    "As before, we're going to simplify the problem by working in a grid world. The symbols that form the grid have a special meaning as they specify the type of the terrain and the cost to enter a grid cell with that type of terrain:\n",
    "\n",
    "```\n",
    "token   terrain    cost \n",
    ".       plains     1\n",
    "*       forest     3\n",
    "^       hills      5\n",
    "~       swamp      7\n",
    "x       mountains  impassible\n",
    "```\n",
    "\n",
    "When you go from a plains node to a forest node it costs 3. When you go from a forest node to a plains node, it costs 1. You can think of the grid as a big graph. Each grid cell (terrain symbol) is a node and there are edges to the north, south, east and west (except at the edges).\n",
    "\n",
    "There are quite a few differences between A\\* Search and Reinforcement Learning but one of the most salient is that A\\* Search returns a plan of N steps that gets us from A to Z, for example, A->C->E->G.... Reinforcement Learning, on the other hand, returns  a *policy* that tells us the best thing to do in **every state.**\n",
    "\n",
    "For example, the policy might say that the best thing to do in A is go to C. However, we might find ourselves in D instead. But the policy covers this possibility, it might say, D->E. Trying this action might land us in C and the policy will say, C->E, etc. At least with offline learning, everything will be learned in advance (in online learning, you can only learn by doing and so you may act according to a known but suboptimal policy).\n",
    "\n",
    "Nevertheless, if you were asked for a \"best case\" plan from (0, 0) to (n-1, n-1), you could (and will) be able to read it off the policy because there is a best action for every state. You will be asked to provide this in your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same costs as before. Note that we've negated them this time because RL requires negative costs and positive rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': -1, '*': -3, '^': -5, '~': -7, 'x': -100000}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs = {\".\": -1, \"*\": -3, \"^\": -5, \"~\": -7, \"x\": -100000}\n",
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a list of offsets for `cardinal_moves`. You'll need to work this into your **actions**, A, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_moves = [(0, -1), (1, 0), (0, 1), (-1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Value Iteration, we require knowledge of the *transition* function, as a probability distribution.\n",
    "\n",
    "The transition function, T, for this problem is 0.70 for the desired direction, and 0.10 each for the other possible directions. That is, if the agent selects \"north\" then 70% of the time, it will go \"north\" but 10% of the time it will go \"east\", 10% of the time it will go \"west\", and 10% of the time it will go \"south\". If agent is at the edge of the map, it simply bounces back to the current state.\n",
    "\n",
    "You need to implement `value_iteration()` with the following parameters:\n",
    "\n",
    "+ world: a `List` of `List`s of terrain (this is S from S, A, T, gamma, R)\n",
    "+ costs: a `Dict` of costs by terrain (this is part of R)\n",
    "+ goal: A `Tuple` of (x, y) stating the goal state.\n",
    "+ reward: The reward for achieving the goal state.\n",
    "+ actions: a `List` of possible actions, A, as offsets.\n",
    "+ gamma: the discount rate\n",
    "\n",
    "you will return a policy: \n",
    "\n",
    "`{(x1, y1): action1, (x2, y2): action2, ...}`\n",
    "\n",
    "Remember...a policy is what to do in any state for all the states. Notice how this is different than A\\* search which only returns actions to take from the start to the goal. This also explains why reinforcement learning doesn't take a `start` state.\n",
    "\n",
    "You should also define a function `pretty_print_policy( cols, rows, policy)` that takes a policy and prints it out as a grid using \"^\" for up, \"<\" for left, \"v\" for down and \">\" for right. Use \"x\" for any mountain or other impassable square. Note that it doesn't need the `world` because the policy has a move for every state. However, you do need to know how big the grid is so you can pull the values out of the `Dict` that is returned.\n",
    "\n",
    "```\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    ">>>>>>v\n",
    "^^^>>>v\n",
    "^^^>>>v\n",
    "^^^>>>G\n",
    "```\n",
    "\n",
    "(Note that that policy is completely made up and only illustrative of the desired output). Please print it out exactly as requested: **NO EXTRA SPACES OR LINES**.\n",
    "\n",
    "* If everything is otherwise the same, do you think that the path from (0,0) to the goal would be the same for both A\\* Search and Q-Learning?\n",
    "* What do you think if you have a map that looks like:\n",
    "\n",
    "```\n",
    "><>>^\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>G\n",
    "```\n",
    "\n",
    "has this converged? Is this a \"correct\" policy? What are the problems with this policy as it is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Callable\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_world(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 0:\n",
    "                result.append(list(line.strip()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"value_iteration\"></a>\n",
    "### value_iteration\n",
    "\n",
    "`value_iteration` performs the stochastic version of the value iteration reinforcement learning algorithm. **Uses**: [transition_sum](#transition_sum), [transition](#transition), [reward_calc](#reward_calc), [initiate_q](#initiate_q), [initiate_space](#initiate_space)\n",
    "\n",
    "* **world** List[List]: 2-D list of spaces representing a world\n",
    "* **costs** Dict: dictionary of the cost to move through any tile\n",
    "* **goal** Tuple: the coordinates of the goal\n",
    "* **rewards** int: the reward for getting to the goal\n",
    "* **actions** List[Tuple]: list of actions allowed\n",
    "* **gamma** float: the discount rate \n",
    "\n",
    "**return**: Dict: the dictionary of the policy found for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(world, costs, goal, rewards, actions, gamma):\n",
    "    goal = (goal[1], goal[0])\n",
    "    v_s = initiate_space(world)\n",
    "    q = np.asarray(initiate_q(world, actions))\n",
    "    r = reward_calc(world, costs, goal, rewards)\n",
    "    tran = transition(actions)\n",
    "    policy = {}\n",
    "    while True:\n",
    "        v_s_last = deepcopy(v_s)\n",
    "        for s1, v1 in enumerate(world):\n",
    "            for s2, v2 in enumerate(v1):\n",
    "                for a, value in enumerate(actions):\n",
    "                    t_sum = transition_sum(v_s_last, s1, s2, tran, world, actions, a)\n",
    "                    if s1 == goal[0] and s2 == goal[1]:\n",
    "                        q[a][s1][s2] = r[s1][s2]\n",
    "                    elif world[s1][s2] == \"x\":\n",
    "                        q[a][s1][s2] = r[s1][s2]\n",
    "                    else:\n",
    "                        q[a][s1][s2] = r[s1][s2] + gamma * t_sum\n",
    "                max_value = -100\n",
    "                for i_q, v_q in enumerate(q):\n",
    "                    if q[i_q][s1][s2] > max_value:\n",
    "                        max_value = q[i_q][s1][s2]\n",
    "                        p = i_q\n",
    "                        ref = (i_q, s1, s2)\n",
    "                    if q[i_q][s1][s2] < -100:\n",
    "                        p = \"x\"\n",
    "                policy[(s1, s2)] = p\n",
    "                v_s[s1][s2] = q[ref]\n",
    "        epsilon = np.max(\n",
    "            [i1 - i2 for i1, i2 in zip(np.asarray(v_s), np.asarray(v_s_last))]\n",
    "        )\n",
    "        if epsilon < 0.001:\n",
    "            break\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transition_sum\"></a>\n",
    "### transition_sum\n",
    "\n",
    "`transition_sum` performs the summation of the potential transitions to different states. Used in the equation to update Q. **Used by**:[value_iteration](#value_iteration)\n",
    "\n",
    "* **v_s_last** List[List]: the list holding the last highest value action for each state\n",
    "* **s1** int: the x coordinate of the current state\n",
    "* **s2** int: the y coordinate of the current state\n",
    "* **tran** List[List]: the chance of a transiton to a different state from a specific action\n",
    "* **world** List[List]: 2-D list of spaces representing a world\n",
    "* **actions** List[Tuple]: list of actions allowed \n",
    "* **a** int: the index of the current action from the list\n",
    "\n",
    "**return**: float: the sum of the products of transitions to different states due to the current action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_sum(v_s_last, s1, s2, tran, world, actions, a):\n",
    "    v_s_last_t_list = []\n",
    "    for a1, v in enumerate(actions):\n",
    "        if world[s1][s2] == \"x\":\n",
    "            v_s_last_t_list.extend([v_s_last[s1][s2]])\n",
    "        elif s1 == 0 and actions[a1][0] < 0:\n",
    "            v_s_last_t_list.extend([v_s_last[s1][s2]])\n",
    "        elif s1 == len(world) - 1 and actions[a1][0] > 0:\n",
    "            v_s_last_t_list.extend([v_s_last[s1][s2]])\n",
    "        elif s2 == 0 and actions[a1][1] < 0:\n",
    "            v_s_last_t_list.extend([v_s_last[s1][s2]])\n",
    "        elif s2 == len(world[0]) - 1 and actions[a1][1] > 0:\n",
    "            v_s_last_t_list.extend([v_s_last[s1][s2]])\n",
    "        else:\n",
    "            v_s_last_t_list.extend([v_s_last[s1 + actions[a1][0]][s2 + actions[a1][1]]])\n",
    "\n",
    "    t_sum = sum([i1 * i2 for i1, i2 in zip(tran[a], v_s_last_t_list)])\n",
    "    return t_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"initiate_space\"></a>\n",
    "### initiate_space\n",
    "\n",
    "`initiate_space` creates a list of lists the same size as the world with all values initialized to 0. **Used by**:[value_iteration](#value_iteration), [initiate_q](#initiate_q), [reward_calc](#reward_calc)\n",
    "\n",
    "* **world** List[List]: 2-D list of spaces representing a world\n",
    "\n",
    "**return**: List[List]: an empty space the same size as the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_space(world):\n",
    "    v_s = []\n",
    "    for i, v in enumerate(world):\n",
    "        temp = [0.0] * (len(world[0]))\n",
    "        v_s.append(temp)\n",
    "    return v_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"initiate_q\"></a>\n",
    "### initiate_q\n",
    "\n",
    "`initiate_q` creates a list of lists the same size as the world with all values initialized to 0 for each available action. **Used by**:[value_iteration](#value_iteration) **Uses**:[initiate_space](#initiate_space)\n",
    "\n",
    "* **world** List[List]: 2-D list of spaces representing a world\n",
    "* **actions** List[Tuple]: list of actions allowed \n",
    "\n",
    "**return**: List[List]: an empty space the same size as the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_q(world, actions):\n",
    "    q = []\n",
    "    for i, a in enumerate(actions):\n",
    "        q.append(initiate_space(world))\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reward_calc\"></a>\n",
    "### reward_calc\n",
    "\n",
    "`reward_calc` creates a static list of lists of rewards for each space in the world. **Used by**:[value_iteration](#value_iteration) **Uses**:[initiate_space](#initiate_space)\n",
    "\n",
    "* **world** List[List]: 2-D list of spaces representing a world\n",
    "* **costs** Dict: dictionary of the cost to move through any tile\n",
    "* **goal** Tuple: the coordinates of the goal\n",
    "* **rewards** int: the reward for getting to the goal\n",
    "\n",
    "**return**: List[List]: a list of rewards for moving through each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_calc(world, costs, goal, rewards):\n",
    "    r = initiate_space(world)\n",
    "    for i1, s1 in enumerate(world):\n",
    "        for i2, s2 in enumerate(s1):\n",
    "            r[i1][i2] = costs[s2]\n",
    "            if (i1, i2) == goal:\n",
    "                r[i1][i2] = rewards\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transition\"></a>\n",
    "### transition\n",
    "\n",
    "`transition` the list representing the chance of a transiton to a different state from a specific action. **Used by**:[value_iteration](#value_iteration)\n",
    "\n",
    "* **actions** List[Tuple]: list of actions allowed \n",
    "\n",
    "**return**: List[List]: a list of the transition chance to a different state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(actions):\n",
    "    t = []\n",
    "    tran = []\n",
    "    for i, a in enumerate(actions):\n",
    "        t.append([0.1] * (len(actions)))\n",
    "    for i, a in enumerate(actions):\n",
    "        t[i][i] = 0.7\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pretty_print_policy\"></a>\n",
    "### pretty_print_policy\n",
    "\n",
    "`pretty_print_policy` prints the resulting policy for each state \n",
    "\n",
    "* **policy** Dict: the policy for each x,y coordinate \n",
    "* **goal** Tuple: the coordinates of the goal\n",
    "\n",
    "**return**: None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_policy(cols, rows, policy, goal):\n",
    "    r = 0\n",
    "    c = 0\n",
    "    p = []\n",
    "    while r < rows:\n",
    "        while c < cols:\n",
    "            if policy[(r, c)] == 0:\n",
    "                p.extend(\"<\")\n",
    "            if policy[(r, c)] == 1:\n",
    "                p.extend(\"v\")\n",
    "            if policy[(r, c)] == 2:\n",
    "                p.extend(\">\")\n",
    "            if policy[(r, c)] == 3:\n",
    "                p.extend(\"^\")\n",
    "            if r == goal[1] and c == goal[0]:\n",
    "                p[c] = \"G\"\n",
    "            if policy[(r, c)] == \"x\":\n",
    "                p.extend(\"x\")\n",
    "            c = c + 1\n",
    "\n",
    "        print(\"\".join(p))\n",
    "        p = []\n",
    "        c = 0\n",
    "        r = r + 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "### Small World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_world = read_world(\"data/small.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = (len(small_world[0]) - 1, len(small_world) - 1)\n",
    "gamma = 0.9\n",
    "\n",
    "small_policy = value_iteration(small_world, costs, goal, reward, cardinal_moves, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v>>>vv\n",
      "vvv>vv\n",
      "vvv>vv\n",
      "vvvxvv\n",
      "v>vvvv\n",
      ">>>>vv\n",
      ">>>>>G\n"
     ]
    }
   ],
   "source": [
    "cols = len(small_world[0])\n",
    "rows = len(small_world)\n",
    "pretty_print_policy(cols, rows, small_policy, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_world = read_world(\"data/large.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = (len(large_world[0]) - 1, len(large_world) - 1)\n",
    "gamma = 0.9\n",
    "\n",
    "large_policy = value_iteration(large_world, costs, goal, reward, cardinal_moves, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v>>>>>>>>>>>>>>vvvv>>>>>>>v\n",
      "v>>>>>>>>>>>>>>vvvxxxxxxxvv\n",
      "vvv^xx>>>>>>>>>vvxxxvvvxxvv\n",
      "vvv<vxxxv>>>>>>>vvvvvvvxxvv\n",
      "vvvvvxxvv>>>>>>>>>vvvvxxxvv\n",
      "vvvvxxvvvv>>>>>>>>>>vvvxvvv\n",
      "vvvxx>>vvvvvxxx>>>>>>vvvvvv\n",
      "vvvv>>vvvvvvvvxxx>>>>>>vvvv\n",
      "vv>>>>vvvvvvvvxx>>>>>>>>vvv\n",
      ">>>>>>>vvvvxxxx>>>>>>>>>vvv\n",
      ">>>>>>vvvvxxxv>>>>vvxxx>vvv\n",
      ">>>>>>vvvxxvv>>>>>vvvxxvvvv\n",
      ">>>>>>vvvxxvv>>>>>>>vx>>vvv\n",
      ">>>>>>>vvvvvv>>>>>>>>>>>vvv\n",
      "v>>^x>>>vvvvvv>>>v>>>^x>vvv\n",
      "vvvxxx>>>vvvxxxv>vvv^xxvvvv\n",
      "vvxx>>>>>>>vvvxxxvvxxxvvvvv\n",
      "vvvxx>>>>>>>>vvvxxxxvvvvvvv\n",
      "vvvxxx>>>>>>>>>vvvvvvvvvvvv\n",
      "vvvvxxx>>>>>>>>>>>v>>>vvvvv\n",
      "vvvvvvxx>>>>^^x>>>>>>>vvvvv\n",
      "v>>vvvvxxx^^xx>>>>>>>>>vvvv\n",
      ">>>>>vvvvxxxx>>>>>>>>>>>vvv\n",
      ">>>>>>vvvvv>>>>^^xx>>>>>vvv\n",
      "vx>>>>>>>vvxxx^^xxvxx>>>>vv\n",
      "vxxx>>>>>>vvxxxxv>vvxxx>>>v\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>G\n"
     ]
    }
   ],
   "source": [
    "cols = len(large_world[0])\n",
    "rows = len(large_world)\n",
    "pretty_print_policy(cols, rows, large_policy, goal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
